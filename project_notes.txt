

#1. clean the data to fill null values or remove them

# discrete with mode, continuous with median or mean

# rank_mode = df1["rank"].mode()[0]
# gre_median = round(df1["gre"].median(), 1)
# df1["rank"] = df1["rank"].fillna(rank_mode)
# df1["gre"] = df1["gre"].fillna(gre_median)
# df["horsepower"].replace(to_replace="?", value=np.nan, inplace=True)
# df["horsepower"].fillna(df.horsepower.median(), inplace=True)

# Nearest neighbour methods
    # from sklearn.impute import KNNImputer
    # knn_imp = KNNImputer(n_neighbors=2, weights="uniform")
    # df2_filled = knn_imp.fit_transform(df2[["gre", "gpa", "rank"]])
    # df2_filled = pd.DataFrame(df2_filled, columns=["gre", "gpa", "rank"])
    # df2_filled.insert(0, "admit", df2["admit"], True)
    # df2_filled.info()


# Linear regression based approach
#871501607754.dkr.ecr.us-west-2.amazonaws.com/dva/pit-functional:jenkins-isd-pit-functional-falcon-40-itest


#2. Remove Outliers before cleaning null values
# Ensure you are not dropping more outliers
import numpy as np


def outliers_iqr(a):
    z=[]
    q1, q3 = np.percentile(a, [25, 75])
    iqr = q3 - q1
    lb = q1 - (iqr * 1.5)
    ub = q3 + (iqr * 1.5)
    x=np.where((a>ub) |(a<lb))
    for i in x:
        z.append(x)
    return(z)

# s = outliers_iqr(df2_filled["gre"])
# s[0][0].size

# s = outliers_iqr(df2_filled["gpa"])
# s[0][0].size

#outlier_free_df = df2.drop([289, 304])



#3. dimensionality reduction or reducing number of columns or Feature engineering
 # 1. Density - so much missing data
 # 2. too much diversity -- one per line like emp id
 # 3.too uniform --

# Reduce columns by finding out the correlation 0.9 to -0.9
# PCA
# from sklearn.decomposition import PCA
# pca =PCA(n_components=1)
#principalComp = pca.fit_transform(data)
# pca.explained_variance_ratio_  will tell you how much percentage each dimension is giving advatage

#4. Normalisation -- used when there is huge variation in the certain column
# x=(x-xmin) /(xmax-xmin)    --- this should convert values of columns to 0 to 1
# ex: displacementmin = df.displacement.min()
# displacementmax = df.displacement.max()
# df["displacement'] = (df.displacement-displacementmin) /(displacementmax - displacementmin)

#5. Standardisation -- Range -3 to +3sigma , also called standard form
# z = (x-mean)/std
# ex: displacement_std = df.displacement.std()
# disp_mean = df.displacment.mean()
# df.displacement = (df["displacement"] - disp_mean)/displacement_std
#

#6 Binning - convert continuous values to discrete
# Percentile 0-10 has value 1 and 10-20  has value 2


#7 One hot encoding : convert discret variables to multiple discret variable
#df2 = pd.get_dummies(df,columns=["origin"])

# then find linear regression

# config paramerters

# input data path
# output component
# discrete components
# PCA - dimnensions reduction - 5
# Random state values -- defaults to prime numbers 0 - 500
# KNN Imputer neighbors value 4
# Higly correlated columns - btwn -0.9 to 0.9
# test size 0.1
# accepatance % of empty data < 10%
# one hot encoding for < 8 unique data
